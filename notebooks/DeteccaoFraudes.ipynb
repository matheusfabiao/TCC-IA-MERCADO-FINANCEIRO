{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignorar avisos\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "# Baixe o data frame em: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "caminho_dados = r'C:\\Users\\Matheus Fabiao\\Desktop\\TCC-IA-MERCADO-FINANCEIRO\\data\\creditcard.csv'\n",
    "base_dados = pd.read_csv(caminho_dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz uma cópia dos dados para manter a base original intacta\n",
    "dados = base_dados.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorando os dados\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo informações sobre o DataFrame\n",
    "dados.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumindo estatísticas descritivas do DataFrame\n",
    "dados.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando se há valores nulos nos dados\n",
    "dados.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo a coluna 'Time', por não ser relevante\n",
    "dados = dados.drop(['Time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando as transações em fraude e não fraude\n",
    "nao_fraudes = dados[dados['Class']==0]\n",
    "print(f'O número de transações válidas é de {len(nao_fraudes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando as transações em fraude e não fraude\n",
    "fraudes = dados[dados['Class']==1]\n",
    "print(f'O número de transações fraudulentas é de {len(fraudes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um conjunto de validação com 15 transações de cada classe\n",
    "validacao_nao_fraudes = nao_fraudes.sample(15)\n",
    "validacao_fraudes = fraudes.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenando os dois conjuntos de validação\n",
    "validacao = pd.concat([validacao_nao_fraudes, validacao_fraudes], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo as transações de validação do DataFrame original\n",
    "dados = dados.drop(validacao_nao_fraudes.index)\n",
    "dados = dados.drop(validacao_fraudes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o DataFrame com os valores reais, para comparar com as previsões\n",
    "validacao_real = validacao.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover a coluna alvo 'Class'\n",
    "validacao = validacao.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificação da contagem das classes\n",
    "# Desequilíbrio entre a minoritária e majoritária\n",
    "dados['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os dados\n",
    "x = dados.drop(['Class'], axis=1)\n",
    "y = dados['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação da biblioteca de oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Utilização do SMOTE para a sobreamostragem\n",
    "smote = SMOTE(random_state=42)\n",
    "X_reamostrado, y_reamostrado = smote.fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo a contagem de rótulos após o SMOTE\n",
    "# Equilíbrio entre a minoritária e majoritária\n",
    "y_reamostrado.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os dados em conjuntos de treinamento e teste\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(X_reamostrado, y_reamostrado, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo de Regressão Logística\n",
    "modelo = LogisticRegression(max_iter=1000)\n",
    "modelo.fit(x_treino, y_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando previsões com o modelo\n",
    "previsao = modelo.predict(x_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando o modelo usando as principais métricas\n",
    "precisao = precision_score(y_teste, previsao)\n",
    "recall = recall_score(y_teste, previsao)\n",
    "f1 = f1_score(y_teste, previsao)\n",
    "acuracia = accuracy_score(y_teste, previsao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo as principais métricas\n",
    "print(f'Precisão: {precisao:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'A acurácia do modelo foi de {acuracia:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de Confusão (avaliação visual do desempenho do modelo)\n",
    "confusion = confusion_matrix(y_teste, previsao)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report (resumo das métricas de classificação)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_teste, previsao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validação cruzada\n",
    "# pontuacoes_validacao_cruzada = cross_val_score(estimator=modelo, X=x_treino, y=y_treino, cv=5)\n",
    "# print(\"Média da Validação Cruzada:\", pontuacoes_validacao_cruzada.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve (trade-off entre precisão e recall)\n",
    "precision, recall, thresholds = precision_recall_curve(y_teste, modelo.predict_proba(x_teste)[:, 1])\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(recall, precision, color='darkorange', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a Curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_teste, previsao)\n",
    "# Calculando a Área sob a Curva ROC (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# Plotando a Curva ROC\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo previsões no conjunto de validação\n",
    "predict = modelo.predict(validacao)\n",
    "\n",
    "# Criando um DataFrame para comparar as classes reais com as previsões\n",
    "dados = pd.DataFrame({'real':validacao_real, 'previsao':predict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo o DataFrame de comparação\n",
    "dados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
